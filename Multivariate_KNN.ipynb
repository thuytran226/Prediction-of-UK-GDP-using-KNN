{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multivariate KNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thuytran226/Prediction-of-UK-GDP-using-KNN/blob/main/Multivariate_KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3QxBut-1x8r"
      },
      "source": [
        "## Import basic libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy7CzhHk3qzr"
      },
      "source": [
        "## Import raw data\n",
        "df_raw = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/multivariate_data.csv') #insert path of data\n",
        "df_raw.head()\n",
        "\n",
        "# Data description\n",
        "    # priv_con: private consumption\n",
        "    # gov_ex : government expenditure\n",
        "    # exp: exports\n",
        "    # imp: imports\n",
        "    # invest: investment rate\n",
        "    # cpi: consumer price index\n",
        "    # prod_i: production index\n",
        "    # unemp: unemployment rate\n",
        "    # rgdp: real gross domestic product \n",
        "    # gr_gdp: growth rate of gross domestic product"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um-NIY0s4I-8"
      },
      "source": [
        "## Convert the format of Time\n",
        "df_raw['Time'] = pd.Series(df_raw['Time'], dtype='string')\n",
        "df_raw['Time'] = df_raw['Time'].str.replace(' ','') \n",
        "df_raw.Year = pd.to_datetime(df_raw['Time'])\n",
        "df_raw['Time'] = pd.PeriodIndex(df_raw.Year, freq='Q').to_timestamp()\n",
        "df_raw['Time'] = df_raw['Time'] + pd.offsets.QuarterEnd(0)\n",
        "# Set Time as index of data frame\n",
        "df_raw= df_raw.set_index('Time')\n",
        "df_raw.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yMoLMyc_zq4"
      },
      "source": [
        "## Convert private consumption, government expenditure, exports, imports, and real GDP to log scale. \n",
        "\n",
        "df_raw['priv_con'] = np.log(df_raw['priv_con'])\n",
        "df_raw['gov_ex'] = np.log(df_raw['gov_ex'])\n",
        "df_raw['exp'] = np.log(df_raw['exp'])\n",
        "df_raw['imp'] = np.log(df_raw['imp'])\n",
        "df_raw['rgdp'] = np.log(df_raw['rgdp'])\n",
        "df_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJptQK1I6-r0"
      },
      "source": [
        "## Take the first order of difference for the whole dataframe. \n",
        "\n",
        "df_trans = df_raw.diff(1).dropna()\n",
        "df_trans = df_trans.rename(columns={'rgdp':'gr_gdp'}) # Note that except real gdp, the column names still remains unchanged to avoid the complexity. \n",
        "df_trans.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXEWVyrYDhFE"
      },
      "source": [
        "## Test the stationary of time series\n",
        "\n",
        "# Import the ADF function\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Perform ADF Test of each time series, in which the hypothesis H0: a unit root is present in time series; H1: no unit root exists\n",
        "def adfuller_test(series, sig=0.05, name=''):\n",
        "    res = adfuller(series, autolag='AIC')    \n",
        "    p_value = round(res[1], 3) \n",
        "\n",
        "    if p_value <= sig:\n",
        "        print(f' {name} : P-Value = {p_value} => Stationary. ')\n",
        "    else:\n",
        "        print(f'{name} : P-Value = {p_value} => Non-stationary.')\n",
        "\n",
        "for name, column in df_trans.iteritems():\n",
        "    adfuller_test(column, name=column.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl_vsEneEu34"
      },
      "source": [
        "# Create a data frame which store all variables and their 4 lags\n",
        "df1=df_trans.copy(deep=True)\n",
        "for i in range(0, len(df1.columns)):\n",
        "  for j in range(1, 5):\n",
        "    df1[df1.columns[i] + '_lag_' + str(j)] =df1[[df1.columns[i]]].shift(j)\n",
        "\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTAu3IqbFpfN"
      },
      "source": [
        "## Arrange the data frame so it has the general form ( x(t-4)[i], x(t-3)[i], x(t-2)[i], x(t-1)[i], x[i], y(t-4), y(t-3), y(t-2), y(t-1), y) \n",
        "# with i is the independent variables. \n",
        "\n",
        "newnames=[]\n",
        "for i in range(0, len(df_trans.columns)):\n",
        "  for j in range(4,0,-1):\n",
        "      newnames.append(df1.columns[len(df_trans.columns)-1+ i*4 + j])\n",
        "  newnames.append(df_trans.columns[i])      \n",
        "df2=df1[newnames]\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZmbUdLKGfLp"
      },
      "source": [
        "## Split the data with train/test ratio of 70/30:  70% for training and 30% for testing\n",
        "# Hide this cell in the case of train/test 80/20\n",
        "\n",
        "# df3:training data -> used for tuning hyperparameter and training the model\n",
        "df3 = df2.iloc[0:round(0.7*len(df2['gr_gdp'])),:]\n",
        "\n",
        "#df3: testing data -> used for perfoming out-of-sample forecasts\n",
        "df4 = df2.iloc[round(0.7*len(df2['gr_gdp'])):len(df2['gr_gdp']) ,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppy7piwXGiba"
      },
      "source": [
        "# ## Split the data with train/test ratio of 80/20:  80% for training and 20% for testing\n",
        "# # Hide this cell in the case of train/test 70/30\n",
        "\n",
        "# # df3:training data -> used for tuning hyperparameter and training the model\n",
        "# df3 = df2.iloc[0:round(0.8*len(df2['gr_gdp'])),:]\n",
        "\n",
        "# #df3: testing data -> used for perfoming out-of-sample forecasts\n",
        "# df4 = df2.iloc[round(0.8*len(df2['gr_gdp'])):len(df2['gr_gdp']) ,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWsDv4UUH6zu"
      },
      "source": [
        "## Create the feature subsets \n",
        "\n",
        "column_list = list(filter(lambda x: not x.endswith('gr_gdp'), df3.columns))\n",
        "column_list\n",
        "featuresubsets = []\n",
        "featuresubsets.append(column_list)\n",
        "for i in [4,3,2]:\n",
        "  if i == 4:\n",
        "    featuresubsets.append(list(filter(lambda x: not x.endswith(str(i)), column_list)))\n",
        "  if i == 3: \n",
        "    featuresubsets.append(list(filter(lambda x: not x.endswith(str(i)) and not x.endswith(str(i+1)), column_list)))\n",
        "  if i == 2:  \n",
        "    featuresubsets.append(list(filter(lambda x: not x.endswith(str(i)) and not x.endswith(str(i+1)) and not x.endswith(str(i+2)), column_list)))\n",
        "len(featuresubsets) \n",
        "featuresubsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTPGZ7CmIM0o"
      },
      "source": [
        "## Perfom the Grid Search with 10-fold Cross-validation to tun the hyperparameters\n",
        "\n",
        "# Import libraries and function for modelling\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from math import sqrt\n",
        "import statistics\n",
        "\n",
        "# Create a list to store the resulst of Gridsearch CV\n",
        "\n",
        "FeatureNames = []\n",
        "NumberOfFeatures = []\n",
        "NumberOfNeighbors = []\n",
        "WeightingScheme = []\n",
        "DistanceMetric = []\n",
        "ValidationAccuracy = []\n",
        "\n",
        "# Separate the features and target values\n",
        "\n",
        "X = df3.iloc[:,:-1]\n",
        "Y = df3['gr_gdp']\n",
        "\n",
        "\n",
        "# GRID SEARCH ALGORITHM\n",
        "\n",
        "# loop throughs all feature subset\n",
        "for num in range(0,len(featuresubsets)):\n",
        "    # modify X by Xnew so that it includes only the selected feature subsets\n",
        "    if type(featuresubsets[num]) == str:\n",
        "        XNew = X[[featuresubsets[num]]].dropna()\n",
        "    else:\n",
        "        XNew = X[list(featuresubsets[num])].dropna()\n",
        "        \n",
        "    # For each feature subsets, create models with every values of K from 2 to 12. \n",
        "    for K in range(2,12):\n",
        "        # For each combination of feature subsets and K, create models with different combinations of weighting schemes and distance metrics\n",
        "        for weight in ['uniform', 'distance']:\n",
        "            for distance in ['euclidean','manhattan']:\n",
        "              \n",
        "                model = KNeighborsRegressor(n_neighbors = K, weights = weight , metric = distance)\n",
        "                \n",
        "                # Creat a list to store the training/validation score of every iteration through Cross-validation process\n",
        "                modeltrain = []\n",
        "                modeltest = []\n",
        "                \n",
        "                # Perform the 10-folds cross-validation process, then store all training/validation scores of each iterations.\n",
        "                cv = KFold(n_splits = 10,shuffle=True, random_state=123)\n",
        "          \n",
        "                for train,test in cv.split(XNew):\n",
        "                    \n",
        "                    xtrain,xtest,ytrain,ytest = XNew.iloc[train],XNew.iloc[test],Y.iloc[train],Y.iloc[test]\n",
        "                    model.fit(xtrain,ytrain)\n",
        "                    modeltest.append(model.score(xtest,ytest))\n",
        "                    \n",
        "                # Store the values of hyperparameters\n",
        "                FeatureNames.append(featuresubsets[num])\n",
        "                NumberOfFeatures.append(len(XNew.columns))\n",
        "                NumberOfNeighbors.append(K)\n",
        "                WeightingScheme.append(weight)\n",
        "                DistanceMetric.append(distance)\n",
        "                ValidationAccuracy.append(statistics.mean(modeltest)) # Take average validation score of all iterations of each model\n",
        "              \n",
        "# Create a summary report of Grid Search with 10-fold CV, then return the top 5 models with highest validation score. \n",
        "SummaryReport = pd.DataFrame()\n",
        "SummaryReport['Feature Names'] = FeatureNames\n",
        "SummaryReport['Number of Features'] = NumberOfFeatures\n",
        "SummaryReport['Number of Neighbors K'] = NumberOfNeighbors\n",
        "SummaryReport['Weighting Scheme'] = WeightingScheme\n",
        "SummaryReport['Distance Metric'] = DistanceMetric\n",
        "SummaryReport['Validation Accuracy'] = ValidationAccuracy\n",
        "SummaryReport.sort_values('Validation Accuracy', ascending=False,inplace=True)\n",
        "SummaryReport = SummaryReport.head(5)\n",
        "SummaryReport.set_index('Feature Names',inplace = True)\n",
        "SummaryReport\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWxowv52I7Al"
      },
      "source": [
        "## Create data for prediction based on the feature selection above\n",
        "\n",
        "#DF for train\n",
        "df_train = df3[['priv_con_lag_1','priv_con','gov_ex_lag_1','gov_ex','exp_lag_1','exp','imp_lag_1','imp','invest_lag_1','invest','prod_i_lag_1',\n",
        "  'prod_i','cpi_lag_1','cpi','unemp_lag_1','unemp','gr_gdp_lag_1','gr_gdp']].dropna()\n",
        "X_train = df_train.iloc[:,:-1]\n",
        "Y_train = df_train['gr_gdp']\n",
        "\n",
        "#DF for test\n",
        "\n",
        "df_test = df_train = df4[['priv_con_lag_1','priv_con','gov_ex_lag_1','gov_ex','exp_lag_1','exp','imp_lag_1','imp','invest_lag_1','invest','prod_i_lag_1',\n",
        "  'prod_i','cpi_lag_1','cpi','unemp_lag_1','unemp','gr_gdp_lag_1','gr_gdp']].dropna()\n",
        "X_test = df_test.iloc[:,:-1]\n",
        "Y_test = df_test['gr_gdp']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5fw-_X9mKhl"
      },
      "source": [
        "## Perform out-of-sample forecast for one-step-ahead using KNN regression\n",
        "\n",
        "# Import the accuracy metrics \n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "# Create features and targets as input and output for training phase\n",
        "X_train = df_train.iloc[:,:-1]\n",
        "Y_train = df_train['gr_gdp']\n",
        "\n",
        "# Create input for prediction, Y_test now is the observed values\n",
        "X_test = df_test.iloc[:,:-1]\n",
        "Y_test = df_test['gr_gdp']\n",
        "    \n",
        "# Perfom out-of-sample forecast\n",
        "\n",
        "#Train/test ratio is 70/30 -> Hide this row if train/test ratio is 80/20\n",
        "KNN = KNeighborsRegressor(n_neighbors = 4, weights = 'uniform', metric = 'manhattan')\n",
        "\n",
        "#Train/test 80/20 -> Hide this row if train/test ratio is 70/30\n",
        "# KNN = KNeighborsRegressor(n_neighbors = 8, weights = 'uniform', metric = 'manhattan')\n",
        "\n",
        "knn = KNN.fit(X_train, Y_train)\n",
        "Y_pred = knn.predict(X_test)\n",
        "\n",
        "# Print the forecast errors\n",
        "\n",
        "print('MAE: ' + str(np.mean(np.abs(Y_test - Y_pred))))\n",
        "print('RMSE: ' + str(np.sqrt(mse(Y_test, Y_pred))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH1YC87undUm"
      },
      "source": [
        "## Create the data frame of results for visualizing a comparision of observed values and predicted values\n",
        "df5=pd.DataFrame(Y_test)\n",
        "df5['knn_pred'] = Y_pred.tolist()\n",
        "df5=df5.sort_index()\n",
        "df5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbAC51g-njRf"
      },
      "source": [
        "## Plot the forecast comparision between observed values and predicted values in case of train/test 70/30\n",
        "#Hide this cell in case of train/test 80/20\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.ylim(-0.04,0.04)\n",
        "plt.plot(df5['gr_gdp'], 'b-', label = 'Actual')\n",
        "plt.plot(df5['knn_pred'], 'r-', label = 'Predicted')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Log Scale')\n",
        "plt.title('Multivariate KNN - Predicted vs. Actual Values (train/test = 70/30)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNkgqXL2nzPt"
      },
      "source": [
        "# ## Plot the forecast comparision between observed values and predicted values in case of train/test 80/20\n",
        "# #Hide this cell in case of train/test 70/30\n",
        "\n",
        "# plt.figure(figsize=(9, 4))\n",
        "# plt.ylim(-0.04,0.04)\n",
        "# plt.plot(df5['gr_gdp'], 'b-', label = 'Actual')\n",
        "# plt.plot(df5['knn_pred'], 'r-', label = 'Predicted')\n",
        "# plt.xlabel('Year')\n",
        "# plt.ylabel('Log Scale')\n",
        "# plt.title('Multivariate KNN - Predicted vs. Actual Values (train/test = 80/20)')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN6cXdRin3vg"
      },
      "source": [
        "## FORECASTING USING VAR MODEL\n",
        "\n",
        "# Train/test ratio is 70/30 -> Hide the these two rows if  train/test ratio is 80/20\n",
        "df_train = df_trans.iloc[0:round(0.7*len(df_trans)),:]\n",
        "df_test = df_trans.iloc[round(0.7*len(df_trans)):len(df_trans),:]\n",
        "\n",
        "# # # Train/test ratio is 80/20 -> Hide the these two rows if  train/test ratio is 70/30\n",
        "# df_train = df_trans.iloc[0:round(0.8*len(df_trans)),:]\n",
        "# df_test = df_trans.iloc[round(0.8*len(df_trans)):len(df_trans),:]\n",
        "\n",
        "# Transform the data into stationary by taking the first order of difference of df_trans ( data frame of growth rate)\n",
        "df_diff = df_train.diff(1).dropna()\n",
        "\n",
        "# Perform ADF Test of each time series again, in which the hypothesis H0: a unit root is present in time series; H1: no unit root exists\n",
        "\n",
        "for name, column in df_diff.iteritems():\n",
        "    adfuller_test(column, name=column.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wo21ojcn4Sw"
      },
      "source": [
        "#Import VAR model\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "\n",
        "model = VAR(df_diff)\n",
        " \n",
        "# Choose the order p of VAR model with maximum lag = 4 and criteria for model selection is AIC\n",
        "for i in range(1,4):\n",
        "    result = model.fit(i)\n",
        "    print('Lag Order =', i)\n",
        "    print('AIC : ', result.aic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_4AszLTbG1R"
      },
      "source": [
        "# Another way of choosing the order p of VAR model with maximum lag = 4 and criteria for model selection is AIC\n",
        "x = model.select_order(maxlags=4)\n",
        "x.summary()\n",
        "\n",
        "# p=4 is the optimal value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgBMvze4I2HO"
      },
      "source": [
        "# Fitting the model with order p=4\n",
        "model_fit = model.fit(4)\n",
        "# View the model summary\n",
        "model_fit.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pzPey7uIst1"
      },
      "source": [
        "## Creat input data for forecasting\n",
        "forecast_input = df_diff.values[-4:]\n",
        "forecast_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mt3aW2cH6On"
      },
      "source": [
        "# Make out-of-sample prediction\n",
        "\n",
        "prediction = pd.DataFrame(model_fit.forecast(y= forecast_input, steps=len(df_test)),index = df_test.index, columns= df_train.columns+'1d')\n",
        "prediction.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAS6-Jr0nvk0"
      },
      "source": [
        "## Since the predicted value now is a first-order difference, we need to invert the transfomation to the original scale. \n",
        "# De-difference of one order is taken by adding the original training data and a cumulative sum of predicted values\n",
        "\n",
        "prediction['var_pred'] = df_train['gr_gdp'].iloc[-1] +  prediction['gr_gdp1d'].cumsum()\n",
        "prediction['var_pred'] \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR2WCfosW9xb"
      },
      "source": [
        "# Add the predicted values from VAR to the data frame to compare with the actual values\n",
        "\n",
        "df5['var_pred'] = prediction['var_pred'].tolist()\n",
        "df5=df5.sort_index()\n",
        "df5.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6HCKUMBSedy"
      },
      "source": [
        "# Print the forecast errors\n",
        "\n",
        "print('MAE: ' + str(np.mean(np.abs(df5['gr_gdp'] - df5['var_pred']))))\n",
        "print('RMSE: ' + str(np.sqrt(mse(df5['gr_gdp'], df5['var_pred']))))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}